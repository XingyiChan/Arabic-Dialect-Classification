{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ass1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npOZNoj5WdOZ",
        "colab_type": "text"
      },
      "source": [
        "## **Xing Yi Chan**\n",
        "## **R00183768**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVldE-uKpHx0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "adcab183-a8e7-42a2-b537-c61687ed06a4"
      },
      "source": [
        "# install farasa\n",
        "!pip install farasapy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting farasapy\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/32/3647a6763dbd2cb4d5777a9a7b0f8443daa2924277518d7a9700617e82c4/farasapy-0.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from farasapy) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from farasapy) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (2020.6.20)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09c2KtLqpuLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import necessary libraries\n",
        "import re\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import nltk\n",
        "from nltk.corpus import stopwords # import stopwords\n",
        "from farasa.segmenter import FarasaSegmenter  # import farasa tokenizer / segmenter\n",
        "from farasa.diacratizer import FarasaDiacritizer # import farasa diacritizer\n",
        "from farasa.stemmer import FarasaStemmer # import farasa stemmer \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLl9cv5a_4BF",
        "colab_type": "text"
      },
      "source": [
        "## **Text preprocessing**\n",
        "Below is the list of text preprocessing methods that will be used to preprocess the data.\n",
        "- Tokenization / segmentation\n",
        "- Removing diacritics\n",
        "- Removing punctuation\n",
        "- Removing stopwords\n",
        "- Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qsl6zj3pzyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import training and testing data\n",
        "training_data = pd.read_csv('/content/drive/My Drive/NLP/dataset/MADAR-Corpus-26-train.tsv', sep= '\\t', \n",
        "                            names=['Sentence', 'City'])\n",
        "testing_data = pd.read_csv('/content/drive/My Drive/NLP/dataset/MADAR-Corpus-26-dev.tsv', sep= '\\t', \n",
        "                           names=['Sentence', 'City'])\n",
        "\n",
        "# seperate training and testing data and target values\n",
        "trainX = training_data['Sentence']\n",
        "trainY = training_data['City']\n",
        "testX = testing_data['Sentence']\n",
        "testY = testing_data['City']\n",
        "\n",
        "# perform lable encoding on both training and testing target values\n",
        "enc = LabelEncoder()\n",
        "trainY = enc.fit_transform(trainY)\n",
        "testY = enc.fit_transform(testY)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kndTJAfRzoDD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "7cfda3b1-c0cd-4d07-f360-5ca81485a5d8"
      },
      "source": [
        "# segmentation / tokenization\n",
        "def tokenization(train, test):\n",
        "    segmenter = FarasaSegmenter()\n",
        "    train_output = train.apply(lambda x: segmenter.segment(x))\n",
        "    test_output = test.apply(lambda x: segmenter.segment(x))\n",
        "    print()\n",
        "    print('Completed segmentation......')\n",
        "\n",
        "    return train_output, test_output\n",
        "\n",
        "trainX, testX = tokenization(trainX, testX)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "perform system check...\n",
            "check java version...\n",
            "Your java version is 11.0 which is compatiple with Farasa \n",
            "check toolkit binaries...\n",
            "Dependencies seem to be satisfied..\n",
            "task [SEGMENT] is initialized in \u001b[34mSTANDALONE \u001b[37mmode...\n",
            "Completed segmentation......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtzFb3IG7h1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "7199778b-fe92-41ff-a63f-152052c19b34"
      },
      "source": [
        "# remove diacritics\n",
        "def remove_diacritics(train, test):\n",
        "    diacritizer = FarasaDiacritizer()\n",
        "    train_output = train.apply(lambda x: diacritizer.diacritize(x))\n",
        "    test_output = test.apply(lambda x: diacritizer.diacritize(x))\n",
        "    print()\n",
        "    print('Completed diacritization......')\n",
        "\n",
        "    return train_output, test_output\n",
        "\n",
        "trainX, testX = remove_diacritics(trainX, testX)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "perform system check...\n",
            "check java version...\n",
            "Your java version is 11.0 which is compatiple with Farasa \n",
            "check toolkit binaries...\n",
            "Dependencies seem to be satisfied..\n",
            "task [DIACRITIZE] is initialized in \u001b[34mSTANDALONE \u001b[37mmode...\n",
            "Completed diacritization......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jYcQxmXIAti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "67f4be67-01f3-41e2-cac6-fcfb76b8cae5"
      },
      "source": [
        "# remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    output = re.sub(r'[^\\w\\s]','', text) # common punctuations\n",
        "    output = re.sub('[،؛؟.]','', text)  # arabic punctuations  \n",
        "    return output\n",
        "\n",
        "for i in range(len(trainX)):\n",
        "    trainX[i] = remove_punctuation(trainX[i])\n",
        "\n",
        "for j in range(len(testX)):\n",
        "    testX[j] = remove_punctuation(testX[j])\n",
        "\n",
        "print('Completed punctuation removal......')"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Completed punctuation removal......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43mFoDv4IYj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "ad2f4eba-4af0-4b04-ca1a-09dfbbb63385"
      },
      "source": [
        "# remove stopwords\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words('arabic')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = [w for w in text if w not in stopwords]\n",
        "    return \"\".join(words)\n",
        "\n",
        "for i in range(len(trainX)):\n",
        "    trainX[i] = remove_stopwords(trainX[i])\n",
        "\n",
        "for j in range(len(testX)):\n",
        "    testX[j] = remove_stopwords(testX[j])\n",
        "\n",
        "print('Completed stopwords removal......')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Completed stopwords removal......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRR1BM3PXclK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "96c82cc7-7431-4c37-eea1-fec5b228168e"
      },
      "source": [
        "# stemming\n",
        "def stemming(train, test):\n",
        "    stemmer = FarasaStemmer()\n",
        "    train_output = train.apply(lambda x: stemmer.stem(x))\n",
        "    test_output = test.apply(lambda x: stemmer.stem(x))\n",
        "    print()\n",
        "    print('Completed stemming...')\n",
        "\n",
        "    return train_output, test_output\n",
        "\n",
        "trainX, testX = stemming(trainX, testX)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "perform system check...\n",
            "check java version...\n",
            "Your java version is 11.0 which is compatiple with Farasa \n",
            "check toolkit binaries...\n",
            "Dependencies seem to be satisfied..\n",
            "task [STEM] is initialized in \u001b[34mSTANDALONE \u001b[37mmode...\n",
            "Completed stemming...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM4-OZejA7CY",
        "colab_type": "text"
      },
      "source": [
        "## **Create tf-idf vector**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIsIQt2vsAJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "b1f0d029-c9ff-4cc5-f7c2-edf1331dc01b"
      },
      "source": [
        "# create tf-idf vectors\n",
        "vectorizer = TfidfVectorizer(max_features=1500, analyzer='word', ngram_range=(1, 2))\n",
        "trainX_tfidf = vectorizer.fit_transform(trainX)\n",
        "testX_tfidf = vectorizer.fit_transform(testX)\n",
        "\n",
        "print('Completed vectorization......')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Completed vectorization......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZopUK0mrBCb2",
        "colab_type": "text"
      },
      "source": [
        "## **System implementation**\n",
        "Four different machine learning classification will be used to classify the data. The models are as below.\n",
        "- Naive Bayes Classifier\n",
        "- Support Vector Machine Classifier\n",
        "- k-Nearest Neighbour Classifier\n",
        "- Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeL-nLTdsGXK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a9d2cd8a-ae62-4a8c-c854-ebe3376a6392"
      },
      "source": [
        "# naive bayes classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(trainX_tfidf, trainY)\n",
        "print('Accuracy: ', nb.score(testX_tfidf, testY)*100)\n",
        "\n",
        "y_pred = nb.predict(testX_tfidf)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 71.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj_GjhU0h9I_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2cdbccec-2f65-4447-9c82-89abedef94a7"
      },
      "source": [
        "# svm classifier\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(trainX_tfidf, trainY)\n",
        "print('Accuracy: ', svm.score(testX_tfidf, testY)*100)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 68.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwZrgRYfQWPD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "7ef9a55d-6bce-4694-bfa6-49451f3440c3"
      },
      "source": [
        "# k-nn classifier\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(trainX_tfidf, trainY)\n",
        "print('Accuracy: ', knn.score(testX_tfidf, testY)*100)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 61.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nTwwVzCQX5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "6d023192-7634-4de9-8eb1-e26ebfa45950"
      },
      "source": [
        "# random forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(trainX_tfidf, trainY)\n",
        "print('Accuracy: ', rf.score(testX_tfidf, testY)*100)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 68.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE-yT3tb6rGq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "544d3d79-fbc0-4146-df76-2e01f0f1de16"
      },
      "source": [
        "# Write the test label and predicted output into seperate files\n",
        "pd.DataFrame(testY).to_csv('/content/drive/My Drive/NLP/dataset/Ass1.GOLD')\n",
        "pd.DataFrame(y_pred).to_csv('/content/drive/My Drive/NLP/dataset/Ass1.PRED')\n",
        "\n",
        "print('Completed writing into file......')"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Completed writing into file......\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}